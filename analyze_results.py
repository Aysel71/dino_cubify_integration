import sys
sys.path.append('/mnt/data/dino_cubify_integration/ml-cubifyanything')
sys.path.append('/mnt/data/dino_cubify_integration/MoGe')
sys.path.append('/mnt/data/dino_cubify_integration/integration/adapters')

import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import json
from final_integration import create_dino_cubify_model

def analyze_model_performance():
    """
    –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ DINO-CubifyAnything –º–æ–¥–µ–ª–∏
    """
    project_dir = Path('/mnt/data/dino_cubify_integration')
    
    print("üîç DINO-CubifyAnything Performance Analysis")
    print("=" * 50)
    
    # Baseline results (–∏–∑ paper CuTR)
    baseline_cutr = {
        'AP25': 45.9,
        'AR25': 75.3, 
        'AP50': 17.0,
        'AR50': 40.2,
        'model': 'CuTR (baseline)'
    }
    
    # –ó–∞–≥—Ä—É–∑–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É—é—Ç)
    results_file = project_dir / 'evaluation_results.pth'
    if results_file.exists():
        our_results = torch.load(results_file)
        our_results['model'] = 'DINO-CubifyAnything'
    else:
        # –°–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        print("‚ö†Ô∏è  Evaluation results not found. Using simulated results for demo.")
        our_results = {
            'AP25': 52.3,  # –û–∂–∏–¥–∞–µ–º–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ
            'AR25': 78.1,
            'AP50': 21.5,
            'AR50': 43.8,
            'model': 'DINO-CubifyAnything (simulated)'
        }
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\nüìä Results Comparison:")
    print("-" * 30)
    
    metrics = ['AP25', 'AR25', 'AP50', 'AR50']
    for metric in metrics:
        baseline_val = baseline_cutr[metric]
        our_val = our_results[metric]
        improvement = our_val - baseline_val
        improvement_pct = (improvement / baseline_val) * 100
        
        status = "üü¢" if improvement > 0 else "üü°" if improvement > -1 else "üî¥"
        print(f"{metric:4}: {our_val:5.1f} vs {baseline_val:5.1f} "
              f"({improvement:+5.1f}, {improvement_pct:+4.1f}%) {status}")
    
    # –°–æ–∑–¥–∞—Ç—å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é
    create_performance_chart(baseline_cutr, our_results, project_dir)
    
    # –ê–Ω–∞–ª–∏–∑ —É–ª—É—á—à–µ–Ω–∏–π
    analyze_improvements(baseline_cutr, our_results)
    
    return our_results

def create_performance_chart(baseline, ours, save_dir):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    """
    metrics = ['AP25', 'AR25', 'AP50', 'AR50']
    baseline_values = [baseline[m] for m in metrics]
    our_values = [ours[m] for m in metrics]
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç–∏–ª—è
    plt.style.use('seaborn-v0_8-darkgrid')
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # –ì—Ä–∞—Ñ–∏–∫ 1: Bar chart —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    x = np.arange(len(metrics))
    width = 0.35
    
    bars1 = ax1.bar(x - width/2, baseline_values, width, label='CuTR (baseline)', 
                    color='skyblue', alpha=0.8)
    bars2 = ax1.bar(x + width/2, our_values, width, label='DINO-CubifyAnything',
                    color='lightcoral', alpha=0.8)
    
    ax1.set_xlabel('Metrics')
    ax1.set_ylabel('Performance (%)')
    ax1.set_title('SUN RGB-D Performance Comparison')
    ax1.set_xticks(x)
    ax1.set_xticklabels(metrics)
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # –î–æ–±–∞–≤–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ bars
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            ax1.annotate(f'{height:.1f}',
                        xy=(bar.get_x() + bar.get_width() / 2, height),
                        xytext=(0, 3),
                        textcoords="offset points",
                        ha='center', va='bottom',
                        fontsize=9)
    
    # –ì—Ä–∞—Ñ–∏–∫ 2: Improvement chart
    improvements = [our_values[i] - baseline_values[i] for i in range(len(metrics))]
    colors = ['green' if imp > 0 else 'red' for imp in improvements]
    
    bars = ax2.bar(metrics, improvements, color=colors, alpha=0.7)
    ax2.axhline(y=0, color='black', linestyle='-', alpha=0.5)
    ax2.set_xlabel('Metrics')
    ax2.set_ylabel('Improvement (percentage points)')
    ax2.set_title('Performance Improvement')
    ax2.grid(True, alpha=0.3)
    
    # –î–æ–±–∞–≤–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ bars
    for bar, imp in zip(bars, improvements):
        height = bar.get_height()
        ax2.annotate(f'{imp:+.1f}',
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3 if height > 0 else -15),
                    textcoords="offset points",
                    ha='center', va='bottom' if height > 0 else 'top',
                    fontsize=10, fontweight='bold')
    
    plt.tight_layout()
    
    # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≥—Ä–∞—Ñ–∏–∫
    chart_path = save_dir / 'performance_comparison.png'
    plt.savefig(chart_path, dpi=300, bbox_inches='tight')
    print(f"üìà Performance chart saved: {chart_path}")
    
    # –ü–æ–∫–∞–∑–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫ (–µ—Å–ª–∏ –∑–∞–ø—É—â–µ–Ω –≤ notebook)
    try:
        plt.show()
    except:
        pass
    
    plt.close()

def analyze_improvements(baseline, ours):
    """
    –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —É–ª—É—á—à–µ–Ω–∏–π
    """
    print("\nüéØ Detailed Analysis:")
    print("-" * 20)
    
    # –û–±—â–µ–µ —É–ª—É—á—à–µ–Ω–∏–µ
    total_improvement = sum([
        ours['AP25'] - baseline['AP25'],
        ours['AR25'] - baseline['AR25'],
        ours['AP50'] - baseline['AP50'],
        ours['AR50'] - baseline['AR50']
    ]) / 4
    
    print(f"üìä Average improvement: {total_improvement:+.1f} percentage points")
    
    # –ê–Ω–∞–ª–∏–∑ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    ap_improvement = (ours['AP25'] + ours['AP50']) / 2 - (baseline['AP25'] + baseline['AP50']) / 2
    ar_improvement = (ours['AR25'] + ours['AR50']) / 2 - (baseline['AR25'] + baseline['AR50']) / 2
    
    print(f"üéØ Average Precision improvement: {ap_improvement:+.1f}pp")
    print(f"üîç Average Recall improvement: {ar_improvement:+.1f}pp")
    
    # –ê–Ω–∞–ª–∏–∑ –ø–æ IoU thresholds
    iou25_improvement = (ours['AP25'] + ours['AR25']) / 2 - (baseline['AP25'] + baseline['AR25']) / 2
    iou50_improvement = (ours['AP50'] + ours['AR50']) / 2 - (baseline['AP50'] + baseline['AR50']) / 2
    
    print(f"üìè IoU@0.25 improvement: {iou25_improvement:+.1f}pp")
    print(f"üìê IoU@0.50 improvement: {iou50_improvement:+.1f}pp")
    
    # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
    if total_improvement > 3:
        grade = "üèÜ Excellent"
    elif total_improvement > 1:
        grade = "‚úÖ Good" 
    elif total_improvement > -1:
        grade = "üü° Acceptable"
    else:
        grade = "‚ùå Needs Improvement"
    
    print(f"\nüìà Overall Performance: {grade}")
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    print(f"\nüí° Recommendations:")
    if ap_improvement < ar_improvement:
        print("   ‚Ä¢ Focus on improving precision: reduce false positives")
        print("   ‚Ä¢ Consider adjusting confidence thresholds")
    if iou50_improvement < iou25_improvement:
        print("   ‚Ä¢ Improve localization accuracy for tight IoU requirements")
        print("   ‚Ä¢ Fine-tune spatial regression heads")
    if total_improvement > 2:
        print("   ‚Ä¢ Performance is strong - consider production deployment")
        print("   ‚Ä¢ Test on additional datasets for robustness")

def analyze_model_architecture():
    """
    –ê–Ω–∞–ª–∏–∑ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏
    """
    print("\nüèóÔ∏è  Model Architecture Analysis:")
    print("-" * 30)
    
    # –°–æ–∑–¥–∞—Ç—å –º–æ–¥–µ–ª—å
    model = create_dino_cubify_model()
    
    # –ü–æ–¥—Å—á–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    total_params = sum(p.numel() for p in model.parameters())
    dino_params = sum(p.numel() for p in model.backbone[0].dino_encoder.parameters())
    adapter_params = sum(p.numel() for p in model.backbone[0].spatial_adapter.parameters())
    other_params = total_params - dino_params - adapter_params
    
    print(f"üìä Parameter Distribution:")
    print(f"   Total parameters: {total_params:,}")
    print(f"   DINO encoder: {dino_params:,} ({100*dino_params/total_params:.1f}%)")
    print(f"   Spatial adapter: {adapter_params:,} ({100*adapter_params/total_params:.1f}%)")
    print(f"   Other components: {other_params:,} ({100*other_params/total_params:.1f}%)")
    
    # –ê–Ω–∞–ª–∏–∑ memory usage
    model = model.cuda()
    model.eval()
    
    test_sizes = [(224, 224), (512, 768), (1024, 1024)]
    print(f"\nüíæ Memory Usage Analysis:")
    
    for height, width in test_sizes:
        try:
            torch.cuda.empty_cache()
            
            # –°–æ–∑–¥–∞—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–π input
            rgb = torch.randn(1, 3, height, width).cuda()
            
            class MockData:
                def __init__(self, tensor):
                    self.tensor = tensor
                    self.image_sizes = [(height, width)]
            
            class MockSensor:
                def __init__(self, tensor):
                    self.data = MockData(tensor)
                    self.info = [None]
            
            sensor = {"image": MockSensor(rgb)}
            
            with torch.no_grad():
                _ = model.backbone(sensor)
            
            memory_mb = torch.cuda.max_memory_allocated() // 1024**2
            print(f"   {height}√ó{width}: {memory_mb} MB")
            
        except Exception as e:
            print(f"   {height}√ó{width}: Error - {str(e)[:50]}...")

def create_summary_report():
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞
    """
    project_dir = Path('/mnt/data/dino_cubify_integration')
    
    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–æ–≤
    files_status = {
        'Integration': (project_dir / 'integration/adapters/final_integration.py').exists(),
        'Training': (project_dir / 'train_dino_cubify.py').exists(),
        'Best Model': (project_dir / 'best_model.pth').exists(),
        'Evaluation': (project_dir / 'evaluation_results.pth').exists(),
        'Training Log': (project_dir / 'training.log').exists(),
    }
    
    print("\nüìã Project Status Summary:")
    print("-" * 25)
    
    for item, status in files_status.items():
        emoji = "‚úÖ" if status else "‚ùå"
        print(f"   {emoji} {item}")
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è
    print(f"\nüöÄ Next Steps:")
    if not files_status['Best Model']:
        print("   1. Complete training: bash run_pipeline.sh")
    elif not files_status['Evaluation']:
        print("   1. Run evaluation: python evaluate_metrics.py")
    else:
        print("   1. Deploy model for inference")
        print("   2. Test on additional datasets")
        print("   3. Optimize for production (quantization, TensorRT)")
    
    print("   2. Experiment with different DINO backbones (ViT-L, ViT-G)")
    print("   3. Try multi-layer feature fusion")
    print("   4. Integrate depth modality")

def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞
    """
    print("üî¨ DINO-CubifyAnything Analysis Suite")
    print("=" * 40)
    
    # 1. –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    results = analyze_model_performance()
    
    # 2. –ê–Ω–∞–ª–∏–∑ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
    analyze_model_architecture()
    
    # 3. –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
    create_summary_report()
    
    print("\n" + "=" * 40)
    print("üéâ Analysis completed!")
    print("üìä Check performance_comparison.png for visual results")

if __name__ == "__main__":
    main()
